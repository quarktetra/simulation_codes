{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Name:        durability_simulation with unrecoverable errors\n",
    "# Purpose:      Monte Carlo simualtion to estimate the probability of data loss in erasure coded systems\n",
    "# Author:      TQ, quarktetra@gmail.com\n",
    "# Created:     25/08/2021\n",
    "# Requires:    Python3\n",
    "#-------------------------------------------------------------------------------\n",
    "# run this from the cmd window as: durability_simulation.py 20 2 1 1 20 50 1  1\n",
    "# in IDE, use command line parameters: 20 2 1 1 20 50 1 1\n",
    "# the last integer enables/disables the simulation.\n",
    "# see https://tetraquark.netlify.app/post/raid_durability/   for details\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "\n",
    "def sifter(systems,p_shards,repair_time,failure_rate,hval):   # feed this with the surviving systems. It will return the instances of data losses, and the systems that might still lose data\n",
    "    instances=0\n",
    "    instances_wuer=0\n",
    "    systems_to_keep=[]\n",
    "    for sysid in range(0,len(systems)):\n",
    "        failuretimes=systems[sysid]\n",
    "        failuretimes=failuretimes[failuretimes<365.25]  # we are interested in the first year\n",
    "        sys_lost_data=0\n",
    "        sys_lost_data_wuer=0\n",
    "        if failuretimes.size>p_shards-1: # for data loss you need to have more than p_shards failures, otherwise the system will not lose any data\n",
    "            failuretimes=np.sort (failuretimes)   # time order the failures for each system. We will carefully count how many will accumulate over time\n",
    "            toberecovered=np.array([failuretimes[0]])    # the first failure enters into the toberecovered bucket.\n",
    "            for findex in range(1,failuretimes.size):\n",
    "                this_time= failuretimes[findex]\n",
    "                toberecovered=toberecovered[toberecovered>this_time-repair_time]   # keep the ones that have not been repaired yet. (this_time< t_failure+repair_time )\n",
    "                toberecovered=np.append(toberecovered,[this_time])  # add the current failure to the toberecovered list.\n",
    "\n",
    "                if len(toberecovered)>p_shards: # at any give point, if the list contains more items than the parity count, data is lost\n",
    "                    sys_lost_data=1\n",
    "                    sys_lost_data_wuer=1\n",
    "                if len(toberecovered)>p_shards-1 and random.random()<=hval:   # pull a random number. If it is smaller than hval, then it is URE failures\n",
    "                    sys_lost_data_wuer=1\n",
    "\n",
    "\n",
    "            if sys_lost_data_wuer==0:\n",
    "                failuretimes[0]=failuretimes[0]+np.random.exponential(failure_rate, 1) #the first failed drive will be replaced. Create a new failure time for it. (add it on top of the current time)\n",
    "                systems_to_keep.append(failuretimes)  # keep track of these systems, they can still lose data if a replacement fails soon enough\n",
    "            instances=instances+  sys_lost_data\n",
    "            instances_wuer=instances_wuer+  sys_lost_data_wuer\n",
    "    return   systems_to_keep,instances,instances_wuer\n",
    "\n",
    "def MTTDL_calc(t_shards, p_shards, failure_rate,repair_time):\n",
    "       d_shards=t_shards-p_shards\n",
    "       cval=failure_rate*(failure_rate/repair_time)**p_shards /((d_shards)*factorial(t_shards)/(factorial(d_shards)*factorial(p_shards)));\n",
    "      # print(\"MTTDL for \"+str(t_shards) +\" total shards with \"+\n",
    "       # str(p_shards)+\" shards, i.e.,\"+ str(d_shards) +\"+\"+str(p_shards)+ \",  \"+str(failure_rate)+\"% AFR,  recovery speed \"+ \"(\"+str(round(repair_time,1)) +\" days). -->\"\n",
    "       # +str(cval)+\" ~\"+str(-math.log10(365/cval)) )\n",
    "       return   cval\n",
    "\n",
    "\n",
    "def MTTDL_calc_uer(t_shards, p_shards, failure_rate,repair_time,hval):\n",
    "       MTTDL_th_cred=MTTDL_calc(t_shards, p_shards-1, failure_rate,repair_time)\n",
    "       MTTDL_th_c=MTTDL_calc(t_shards, p_shards, failure_rate,repair_time)\n",
    "       invMTTL= 1/MTTDL_th_c + hval/MTTDL_th_cred\n",
    "       return 1/invMTTL;\n",
    "def h_calc(d_shards,uer,d_cap):\n",
    "\n",
    "       return 1-math.exp(-uer*d_shards*d_cap*8/1000)\n",
    "\n",
    "\n",
    "def simulate(t_shards, p_shards, afr,uer, d_cap, r_speed,simulation_size_scale,simulation_enabled):\n",
    "    d_shards =t_shards -p_shards\n",
    "    repair_time=10**6*d_cap/r_speed/(60*60*24)\n",
    "    failure_rate= -365.25/math.log(1-afr/100)     # this is in 1/days\n",
    "    hval= h_calc(d_shards,uer,d_cap)\n",
    "    MTTDL_th=MTTDL_calc(t_shards,p_shards, failure_rate,repair_time)\n",
    "    MTTDL_th_uer=MTTDL_calc_uer(t_shards,p_shards, failure_rate,repair_time,hval)\n",
    "\n",
    "    nines_th= -math.log10(365/MTTDL_th)\n",
    "    nines_th_uer= -math.log10(365/MTTDL_th_uer)\n",
    "    sim_size=simulation_size_scale*max(10000,min(50*10**math.ceil(nines_th) ,40000000) )\n",
    "    if p_shards>1:\n",
    "        ptext=\" parities\"\n",
    "    else:\n",
    "        ptext=\" parity\"\n",
    "    if simulation_enabled:\n",
    "        print(\"Running \"+str(sim_size)+\" simulations: \"+str(t_shards) +\" total shards with \"+\n",
    "            str(p_shards)+ptext+\", i.e.,\"+ str(d_shards) +\"+\"+str(p_shards)+ \",  \"+str(afr)+\"% AFR, uer=\"+ str(uer)+\" * 10^-{15}, \"+str(d_cap)+\"TB drive capacity, and \"+\n",
    "            str(r_speed)+\"MB/s recovery speed \"+ \"(\"+str(round(repair_time,1)) +\" days).\" )\n",
    "    print(  \"prob of UER=\"+str(round(hval,2)   )    +     \". Theoretical predictions:  NoN= \"+    str(round(nines_th,2))+\" nines, NoN wUER= \" +    str(round(nines_th_uer,2))+\" nines. \" )\n",
    "\n",
    "\n",
    "    #initiate the full simulation data\n",
    "    if simulation_enabled:\n",
    "        systems=[]\n",
    "        for sysid in range(0,sim_size):\n",
    "            systems.append(np.random.exponential(failure_rate, t_shards))\n",
    "\n",
    "        totalinstances=0\n",
    "        totalinstances_wuer=0\n",
    "        while len(systems)>0:\n",
    "            returned=sifter(systems,p_shards,repair_time,failure_rate,hval)\n",
    "            systems=returned[0]\n",
    "            totalinstances=totalinstances+returned[1]\n",
    "            totalinstances_wuer=totalinstances_wuer+returned[2]\n",
    "\n",
    "       # print(\"Simulated \"+str(sim_size)+ \" systems with \" +str(totalinstances ))\n",
    "        if totalinstances>0:\n",
    "            print(\"Simulation Results:\"+str(totalinstances )+\"  data loss instances: NoN= \"\n",
    "            +str(round(-math.log10(totalinstances/sim_size),2))+\" nines, NoN wUER= \"+ str(round(-math.log10(totalinstances_wuer/sim_size),2))+ \" nines\" )\n",
    "        else:\n",
    "            print(\"No failures detected. Try to increase the simulation size!\")\n",
    "\n",
    "     #with one sigma=\"+str(round(1/(totalinstances**0.5),2))\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('number_of_total_shards', type=int),\n",
    "    parser.add_argument('number_of_parity_shards', type=int),\n",
    "    parser.add_argument('annual_failure_rate_in_pct', type=float),\n",
    "    parser.add_argument('uer', type=float),   # in the units of 10^{-15}\n",
    "    parser.add_argument('drive_capacity_in_TB', type=float),\n",
    "    parser.add_argument('recovery_speed_in_MBps', type=float),\n",
    "    parser.add_argument('simulation_size_scale', type=int),\n",
    "    parser.add_argument('simulation_enabled', type=int),\n",
    "    args = parser.parse_args()\n",
    "    simulate(args.number_of_total_shards, args.number_of_parity_shards, args.annual_failure_rate_in_pct,args.uer, args.drive_capacity_in_TB, args.recovery_speed_in_MBps,args.simulation_size_scale,args.simulation_enabled)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
