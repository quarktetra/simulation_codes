{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10000000 simulations: 20 total shards with 2 parities, i.e.,18+2,  2% AFR, 20TB drive capacity, and 50MB/s recovery speed (4.6 days). Theoretical prediction is 5.34 nines. \n",
      "Simulation Results:48  data loss instances: that is 5.32 nines, with one sigma=0.14\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Name:        durability_simulation\n",
    "# Purpose:      Monte Carlo simualtion to estimate the probability of data loss in erasure coded systems\n",
    "# Author:      TQ, quarktetra@gmail.com\n",
    "# Created:     25/08/2021\n",
    "# Requires: Python3\n",
    "#-------------------------------------------------------------------------------\n",
    "# run this from the cmd window as: durability_simulation.py 20 2 1 20 50 1\n",
    "# in IDE, use command line parameters: 20 2 1 20 50 1\n",
    "# see https://tetraquark.netlify.app/post/raid_durability/   for details\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "\n",
    "def sifter(systems,p_shards,repair_time,failure_rate):   # feed this with the surviving systems. It will return the instances of data losses, and the systems that might still lose data\n",
    "    instances=0\n",
    "    systems_to_keep=[]\n",
    "    for sysid in range(0,len(systems)):\n",
    "        failuretimes=systems[sysid]\n",
    "        failuretimes=failuretimes[failuretimes<365.25]  # we are interested in the first year\n",
    "        sys_lost_data=0\n",
    "        if failuretimes.size>p_shards: # for data loss you need to have more than p_shards failures, otherwise the system will not lose any data\n",
    "            failuretimes=np.sort (failuretimes)   # time order the failures for each system. We will carefully count how many will accumulate over time\n",
    "            toberecovered=np.array([failuretimes[0]])    # the first failure enters into the toberecovered bucket.\n",
    "            for findex in range(1,failuretimes.size):\n",
    "                this_time= failuretimes[findex]\n",
    "                toberecovered=toberecovered[toberecovered>this_time-repair_time]   # keep the ones that have not been repaired yet. (this_time< t_failure+repair_time )\n",
    "                toberecovered=np.append(toberecovered,[this_time])  # add the current failure to the toberecovered list.\n",
    "\n",
    "                if len(toberecovered)>p_shards: # at any give point, if the list contains more items than the parity count, data is lost\n",
    "                    sys_lost_data=1\n",
    "            if sys_lost_data==0 or 0:\n",
    "                failuretimes[0]=failuretimes[0]+np.random.exponential(failure_rate, 1) #the first failed drive will be replaced. Create a new failure time for it. (add it on top of the current time)\n",
    "                systems_to_keep.append(failuretimes)  # keep track of these systems, they can still lose data if a replacement fails soon enough\n",
    "            instances=instances+  sys_lost_data\n",
    "    return   systems_to_keep,instances\n",
    "\n",
    "def simulate(t_shards, p_shards, afr, d_cap, r_speed,simulation_size_scale):\n",
    "    d_shards =t_shards -p_shards\n",
    "    repair_time=10**6*d_cap/r_speed/(60*60*24)\n",
    "    failure_rate= -365.25/math.log(1-afr/100)     # this is in 1/days\n",
    "    MTTDL_th=failure_rate*(failure_rate/repair_time)**p_shards /((d_shards)*factorial(t_shards)/(factorial(d_shards)*factorial(p_shards)));\n",
    "    nines_th= -math.log10(365/MTTDL_th)\n",
    "    sim_size=simulation_size_scale*max(10000,min(50*10**math.ceil(nines_th) ,20000000) )\n",
    "    if p_shards>1:\n",
    "        ptext=\" parities\"\n",
    "    else:\n",
    "        ptext=\" parity\"\n",
    "    print(\"Please Wait: Running \"+str(sim_size)+\" simulations: \"+str(t_shards) +\" total shards with \"+\n",
    "        str(p_shards)+ptext+\", i.e.,\"+ str(d_shards) +\"+\"+str(p_shards)+ \",  \"+str(afr)+\"% AFR, \"+str(d_cap)+\"TB drive capacity, and \"+\n",
    "        str(r_speed)+\"MB/s recovery speed \"+ \"(\"+str(round(repair_time,1)) +\" days). Theoretical prediction is \"+\n",
    "        str(round(nines_th,2))+\" nines. \" )\n",
    "\n",
    "\n",
    "    #initiate the full simulation data\n",
    "    systems=[]\n",
    "    for sysid in range(0,sim_size):\n",
    "        systems.append(np.random.exponential(failure_rate, t_shards))\n",
    "\n",
    "    totalinstances=0\n",
    "    while len(systems)>0:\n",
    "        returned=sifter(systems,p_shards,repair_time,failure_rate)\n",
    "        systems=returned[0]\n",
    "        totalinstances=totalinstances+returned[1]\n",
    "\n",
    "   # print(\"Simulated \"+str(sim_size)+ \" systems with \" +str(totalinstances ))\n",
    "    if totalinstances>0:\n",
    "        print(\"Simulation Results:\"+str(totalinstances )+\"  data loss instances: that is \"+str(round(-math.log10(totalinstances/sim_size),2))+\" nines, with one sigma=\"\n",
    "        +str(round(1/(totalinstances**0.5),2)))\n",
    "    else:\n",
    "        print(\"No failures detected. Try to increase the simulation size!\")\n",
    "\n",
    "class Args:\n",
    "  number_of_total_shards = 20\n",
    "  number_of_parity_shards = 2\n",
    "  annual_failure_rate_in_pct = 2\n",
    "  drive_capacity_in_TB = 20\n",
    "  recovery_speed_in_MBps=50  \n",
    "  simulation_size_scale=1\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('number_of_total_shards', type=int),\n",
    "    parser.add_argument('number_of_parity_shards', type=int),\n",
    "    parser.add_argument('annual_failure_rate_in_pct', type=float),\n",
    "    parser.add_argument('drive_capacity_in_TB', type=float),\n",
    "    parser.add_argument('recovery_speed_in_MBps', type=float),\n",
    "    parser.add_argument('simulation_size_scale', type=int),\n",
    "    #args = parser.parse_args() # disable this if you do not want to use command line arguments. Enable the next line to use the inputs in Args class  \n",
    "    args=Args()\n",
    "    simulate(args.number_of_total_shards, args.number_of_parity_shards, args.annual_failure_rate_in_pct, args.drive_capacity_in_TB, args.recovery_speed_in_MBps,args.simulation_size_scale)\n",
    "\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
